{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/DoctorGPT-colab/blob/main/DoctorGPT_colab.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!apt -y install -qq aria2\n",
        "!pip install -q transformers gradio bitsandbytes accelerate\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/medllama2_7b/resolve/main/pytorch_model-00001-of-00002.bin -d /content/medllama2_7b -o pytorch_model-00001-of-00002.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/medllama2_7b/resolve/main/pytorch_model-00002-of-00002.bin -d /content/medllama2_7b -o pytorch_model-00002-of-00002.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/medllama2_7b/raw/main/model.safetensors.index.json -d /content/medllama2_7b -o model.safetensors.index.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/medllama2_7b/raw/main/special_tokens_map.json -d /content/medllama2_7b -o special_tokens_map.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/medllama2_7b/resolve/main/tokenizer.model -d /content/medllama2_7b -o tokenizer.model\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/medllama2_7b/raw/main/tokenizer_config.json -d /content/medllama2_7b -o tokenizer_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/medllama2_7b/raw/main/config.json -d /content/medllama2_7b -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/medllama2_7b/raw/main/generation_config.json -d /content/medllama2_7b -o generation_config.json\n",
        "\n",
        "import gradio as gr\n",
        "import transformers\n",
        "from torch import bfloat16\n",
        "from threading import Thread\n",
        "from gradio.themes.utils.colors import Color\n",
        "\n",
        "model_id = \"/content/medllama2_7b\"\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
        "model_config = transformers.AutoConfig.from_pretrained(model_id)\n",
        "\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "prompts = [\"You are a helpful AI Doctor.\"]\n",
        "\n",
        "def prompt_build(system_prompt, user_inp, hist):\n",
        "    prompt = f\"\"\"### System:\\n{system_prompt}\\n\\n\"\"\"\n",
        "    \n",
        "    for pair in hist:\n",
        "        prompt += f\"\"\"### User:\\n{pair[0]}\\n\\n### Assistant:\\n{pair[1]}\\n\\n\"\"\"\n",
        "\n",
        "    prompt += f\"\"\"### User:\\n{user_inp}\\n\\n### Assistant:\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def chat(user_input, history, system_prompt):\n",
        "\n",
        "    prompt = prompt_build(system_prompt, user_input, history)\n",
        "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    streamer = transformers.TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generate_kwargs = dict(\n",
        "        model_inputs,\n",
        "        streamer=streamer,\n",
        "        #max_new_tokens=512, # will override \"max_len\" if set.\n",
        "        max_length=2048,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        temperature=0.8,\n",
        "        top_k=50\n",
        "    )\n",
        "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "\n",
        "    model_output = \"\"\n",
        "    for new_text in streamer:\n",
        "        model_output += new_text\n",
        "        yield model_output\n",
        "    return model_output\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    dropdown = gr.Dropdown(choices=prompts, label=\"Type your own or select a system prompt\", value=\"You are a helpful AI Doctor.\", allow_custom_value=True)\n",
        "    chatbot = gr.ChatInterface(fn=chat, additional_inputs=[dropdown])\n",
        "\n",
        "demo.queue(api_open=False).launch(show_api=False, share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
